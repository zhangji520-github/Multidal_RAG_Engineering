"""Milvuså‘é‡æ•°æ®åº“æ“ä½œæ¨¡å—.
æä¾›Milvuså‘é‡æ•°æ®åº“çš„é›†åˆåˆ›å»ºã€è¿æ¥å’Œæ–‡æ¡£æ·»åŠ åŠŸèƒ½ã€‚
æ”¯æŒç¨ å¯†å‘é‡å’Œç¨€ç–å‘é‡çš„æ··åˆç´¢å¼•ã€‚
"""
import os
import sys
from typing import List, Optional, Dict

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from splitters.splitter_md import MarkdownDirSplitter
from langchain_core.documents import Document
from utils.embeddings_utils import process_item_with_guard, limiter, RETRY_ON_429, MAX_429_RETRIES, BASE_BACKOFF
from langchain_milvus import Milvus
from pymilvus import DataType, Function, FunctionType, MilvusClient
from env_utils import COLLECTION_NAME, MILVUS_URI, CONTEXT_COLLECTION_NAME
from utils.embeddings_utils import image_to_base64
from utils.common_utils import get_surrounding_text_content
from langchain_core.messages import HumanMessage  
import logging
import time
import random
from llm_utils import qwen3_max



# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MilvusVectorSave:
    """
    1.æ•°æ®åº“çš„ Milvus é›†åˆ dataknowledge_collection åœ¨schemaä¸­ä¿ç•™äº†æœªæ¥éœ€è¦çš„å­—æ®µ ä½¿ç”¨æ··åˆç´¢å¼•ï¼ŒåŒ…å«ç¨€ç–å‘é‡å’Œç¨ å¯†å‘é‡å­—æ®µ 
    2.åˆ›å»ºä¸Šä¸‹æ–‡çš„ Milvus é›†åˆ multimodal_rag_context_collection 
    """
    def __init__(self):
        # ç±»å‹æ³¨è§£ï¼šæ˜ç¡®å£°æ˜å±æ€§ç±»å‹ï¼Œæä¾›IDEæ™ºèƒ½æç¤ºå’Œç±»å‹æ£€æŸ¥
        self.vector_stored_saved: Optional[Milvus] = None
        self.client = MilvusClient(uri=MILVUS_URI, user='root', password='Milvus')

    def create_dataknowledge_collection(self,collection_name: str = COLLECTION_NAME, uri: str = MILVUS_URI, is_first: bool = False):
        """åˆ›å»ºä¸€ä¸ªcollection milvus + langchain"""

        # 2. å®šä¹‰schema
        schema = self.client.create_schema()

        schema.add_field("id", DataType.INT64, is_primary=True, auto_id=True, description="ä¸»é”®")
        schema.add_field("category", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'embedding_type'")     
        schema.add_field("filename", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'source',æ–‡ä»¶å,å¸¦åç¼€")     
        schema.add_field("filetype", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'filetype',pdfæˆ–è€…md")     

        schema.add_field("title", DataType.VARCHAR, max_length=1000, enable_analyzer=True, 
                        analyzer_params={'tokenizer': 'jieba', 'filter': ['cnalphanumonly']}, description="å¯¹åº”å…ƒæ•°æ®çš„Header")
        schema.add_field("text", DataType.VARCHAR, max_length=10000, enable_analyzer=True,
                        analyzer_params={'tokenizer': 'jieba', 'filter': ['cnalphanumonly']}, description="å¯¹åº”æ¯ä¸ªæ–‡æœ¬å—çš„å†…å®¹") 
        schema.add_field("image_path", DataType.VARCHAR, max_length=2000, description="å›¾ç‰‡æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ï¼Œä»…å›¾ç‰‡ç±»å‹æ•°æ®ä½¿ç”¨")

        schema.add_field("title_sparse", DataType.SPARSE_FLOAT_VECTOR, description="æ ‡é¢˜çš„ç¨€ç–å‘é‡åµŒå…¥")
        schema.add_field("text_content_sparse", DataType.SPARSE_FLOAT_VECTOR, description="æ–‡æ¡£å—çš„ç¨€ç–å‘é‡åµŒå…¥")
        schema.add_field("text_content_dense", DataType.FLOAT_VECTOR, dim=1024, description="æ–‡æ¡£å—çš„ç¨ å¯†å‘é‡åµŒå…¥")

        logger.info(f'ğŸ¶æ·»åŠ schemaå®Œæˆ,å…±æ·»åŠ {len(schema.fields)}ä¸ªå­—æ®µ')

        # 3 ç¨€ç–å‘é‡éœ€è¦çš„bm25å‡½æ•°

        title_bm25_function = Function(
            name = "title_bm25_emb",
            input_field_names=["title"], # éœ€è¦è¿›è¡Œæ–‡æœ¬åˆ°ç¨€ç–å‘é‡è½¬æ¢çš„ VARCHAR å­—æ®µåç§°ã€‚
            output_field_names=["title_sparse"], # å­˜å‚¨å†…éƒ¨ç”Ÿæˆç¨€ç–å‘é‡çš„å­—æ®µåç§°ã€‚
            function_type=FunctionType.BM25 # è¦ä½¿ç”¨çš„å‡½æ•°ç±»å‹ã€‚
        )
        schema.add_function(title_bm25_function)          # bm25 æ­¤åŠŸèƒ½ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­è¨€æ ‡è¯†è‡ªåŠ¨åº”ç”¨ç›¸åº”çš„åˆ†æå™¨

        content_bm25_function = Function(
            name = "text_content_bm25_emb",
            input_field_names=["text"], # éœ€è¦è¿›è¡Œæ–‡æœ¬åˆ°ç¨€ç–å‘é‡è½¬æ¢çš„ VARCHAR å­—æ®µåç§°ã€‚
            output_field_names=["text_content_sparse"], # å­˜å‚¨å†…éƒ¨ç”Ÿæˆç¨€ç–å‘é‡çš„å­—æ®µåç§°ã€‚
            function_type=FunctionType.BM25 # è¦ä½¿ç”¨çš„å‡½æ•°ç±»å‹ã€‚
        )
        schema.add_function(content_bm25_function)          # bm25 æ­¤åŠŸèƒ½ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­è¨€æ ‡è¯†è‡ªåŠ¨åº”ç”¨ç›¸åº”çš„åˆ†æå™¨

        # 4 åˆ›å»ºç´¢å¼•å‚æ•°å¯¹è±¡
        try:
            logger.info("å¼€å§‹åˆ›å»ºç´¢å¼•å‚æ•°...")
            index_params = self.client.prepare_index_params()

            # ä¸»é”®ç´¢å¼•
            index_params.add_index(
                field_name="id",
                index_type="AUTOINDEX",
            )

            # ç¨€ç–å‘é‡ç´¢å¼• - æ ‡é¢˜
            index_params.add_index(
                field_name="title_sparse",
                index_type="SPARSE_INVERTED_INDEX",
                metric_type="BM25",
                params={
                    "inverted_index_algo": "DAAT_MAXSCORE",  # ç®—æ³•é€‰æ‹©
                    "bm25_k1": 1.2,  # è¯é¢‘é¥±å’Œåº¦æ§åˆ¶å‚æ•°
                    "bm25_b": 0.75  # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–å‚æ•°
                }
            )

            # ç¨€ç–å‘é‡ç´¢å¼• -æ–‡æœ¬å—
            index_params.add_index(
                field_name="text_content_sparse",
                index_type="SPARSE_INVERTED_INDEX",
                metric_type="BM25",
                params={
                    "inverted_index_algo": "DAAT_MAXSCORE",  # ç®—æ³•é€‰æ‹©
                    "bm25_k1": 1.2,  # è¯é¢‘é¥±å’Œåº¦æ§åˆ¶å‚æ•°
                    "bm25_b": 0.75  # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–å‚æ•°
                }
            )

            # ç¨ å¯†å‘é‡ç´¢å¼• - æ–‡æœ¬å—
            index_params.add_index(
                field_name="text_content_dense",
                index_type="HNSW",  # é€‚åˆç¨ å¯†å‘é‡çš„ç´¢å¼•ç±»å‹
                metric_type="COSINE",  # ä½™å¼¦ç›¸ä¼¼åº¦
                params={
                    "M": 16,  # HNSWå›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
                    "efConstruction": 200  # æ„å»ºç´¢å¼•æ—¶çš„æœç´¢å€™é€‰æ•°
                }
            )

            logger.info("ğŸ¶æˆåŠŸæ·»åŠ ç¨€ç–å‘é‡ç´¢å¼•å’Œç¨ å¯†å‘é‡ç´¢å¼•")

        except Exception as e:
            logger.error(f"ğŸ¶åˆ›å»ºç´¢å¼•å‚æ•°å¤±è´¥: {e}")

        #  5. åˆ›å»ºé›†åˆ
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å…ˆé‡Šæ”¾collectionï¼Œç„¶åå†åˆ é™¤ç´¢å¼•å’Œé›†åˆ  
        if is_first:  
            if COLLECTION_NAME in self.client.list_collections():
                self.client.release_collection(collection_name=COLLECTION_NAME)
                # self.client.drop_index(collection_name=COLLECTION_NAME, index_name="sparse_inverted_index")
                # self.client.drop_index(collection_name=COLLECTION_NAME, index_name="dense_vector_index")
                self.client.drop_collection(collection_name=COLLECTION_NAME)

        self.client.create_collection(
            collection_name=COLLECTION_NAME,
            schema=schema,
            index_params=index_params,
        )
        logger.info(f"ğŸ¶æˆåŠŸåˆ›å»ºé›†åˆ: {COLLECTION_NAME}")
    
    def create_context_collection(self,collection_name: str = CONTEXT_COLLECTION_NAME, uri: str = MILVUS_URI, is_first: bool = False):
        """åˆ›å»ºä¸€ä¸ªcollection milvus + langchain"""

        schema = self.client.create_schema()
        schema.add_field(field_name='id', datatype=DataType.INT64, is_primary=True, auto_id=True, description="ä¸»é”®")
        # æŸä¸€æ¡èŠå¤©è®°å½•çš„æ–‡æœ¬
        schema.add_field(field_name='context_text', datatype=DataType.VARCHAR, max_length=6000, enable_analyzer=True,
                        analyzer_params={"tokenizer": "jieba", "filter": ["cnalphanumonly"]}, description="æŸä¸€æ¡èŠå¤©è®°å½•çš„ä¸Šä¸‹æ–‡èŠå¤©è®°å½•")
        schema.add_field(field_name='user', datatype=DataType.VARCHAR, max_length=1000, nullable=True, description="ç”¨æˆ·å")
        schema.add_field(field_name='timestamp', datatype=DataType.INT64, nullable=True, description="ç”Ÿæˆè¿™æ¡èŠå¤©è®°å½•çš„æ—¶é—´æˆ³")
        schema.add_field(field_name='message_type', datatype=DataType.VARCHAR, max_length=100, nullable=True, description="è¿™æ¡èŠå¤©è®°å½•çš„ç±»å‹")
        schema.add_field(field_name='context_sparse', datatype=DataType.SPARSE_FLOAT_VECTOR, description="ä¸Šä¸‹æ–‡çš„ç¨€ç–å‘é‡åµŒå…¥")
        schema.add_field(field_name='context_dense', datatype=DataType.FLOAT_VECTOR, dim=1024, description="ä¸Šä¸‹æ–‡çš„ç¨ å¯†å‘é‡åµŒå…¥")

        bm25_function = Function(
            name='text_bm25_emb',         # Function name
            input_field_names=['context_text'],
            output_field_names=['context_sparse'],
            function_type=FunctionType.BM25
        )
        schema.add_function(bm25_function)

        index_params = self.client.prepare_index_params()
        # ä¸Šä¸‹æ–‡çš„ ç¨€ç–å‘é‡ç´¢å¼•
        index_params.add_index(
            field_name='context_sparse',
            index_type='SPARSE_INVERTED_INDEX',
            metric_type='BM25',
            params={
                'inverted_index_algo': 'DAAT_MAXSCORE',
                'bm25_k1': 1.2,
                'bm25_b': 0.75
            }
        )
        # ä¸Šä¸‹æ–‡çš„å¯†é›†å‘é‡ç´¢å¼•
        index_params.add_index(
            field_name='context_dense',
            index_type='HNSW',
            metric_type='COSINE',
            params={
                'M': 16,
                'efConstruction': 200
            }
        )

        # åˆ›å»ºé›†åˆ
        if is_first:
            if CONTEXT_COLLECTION_NAME in self.client.list_collections():
                self.client.release_collection(collection_name=CONTEXT_COLLECTION_NAME)
                self.client.drop_collection(collection_name=CONTEXT_COLLECTION_NAME)
        self.client.create_collection(
            collection_name=CONTEXT_COLLECTION_NAME,
            schema=schema,
            index_params=index_params,
        )
        logger.info(f"ğŸ¶æˆåŠŸåˆ›å»ºé›†åˆ: {CONTEXT_COLLECTION_NAME}")

    @staticmethod
    def doc_to_dict(docs: List[Document]) -> List[Dict]:
        """
        å°†Documentåˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼çš„å­—å…¸
        Args:
            docs: åŒ…å« Document å¯¹è±¡çš„åˆ—è¡¨  Document(page_content='', metadata={'source': 'pdf', 'embedding_type': 'text/image/video'}, 'image_path': 'path/to/image.jpg', 'Header1':'...', 'Header2':'...', 'Header3':'...')
        Returns:
            List[Dict]: æŒ‡å®šæ ¼å¼çš„å­—å…¸åˆ—è¡¨
        """
        result_dict = []

        for doc in docs:
            # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸å­˜å‚¨å½“å‰æ–‡æ¡£ä¿¡æ¯
            doc_dict = {}
            metadata = doc.metadata
            
            # 1. æå–text (ä»…å½“embedding_typeä¸ºtext)
            if metadata.get('embedding_type') == 'text':
                doc_dict['text'] = doc.page_content
            else:
                doc_dict['text'] = ''       # # å›¾ç‰‡ç±»å‹åˆå§‹è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
            
            # 2. æå– category (embedding_type)
            doc_dict['category'] = metadata.get('embedding_type', '')
            
            # 3. æå– filename å’Œ filetype (pdf/md ä¹Ÿå°±æ˜¯ sourceçš„æ–‡ä»¶ååç¼€)
            source = metadata.get('source', '')
            doc_dict['filename'] = source
            _, ext = os.path.splitext(source)
            doc_dict['filetype'] = ext[1:].lower() if ext.startswith('.') else ext.lower()
            
            # 4.  å›¾ç‰‡ä¸“ç”¨å¤„ç† æå– image_path (ä»…å½“ embedding_type ä¸º 'image' æ—¶)
            if metadata.get('embedding_type') == 'image':
                doc_dict['image_path'] = doc.page_content        # å›¾ç‰‡è·¯å¾„å­˜å‚¨åœ¨image_path
                doc_dict['text'] = 'å›¾ç‰‡'  # è¦†ç›–ä¹‹å‰çš„ç©ºå­—ç¬¦ä¸²ï¼Œè®¾ç½®ä¸º"å›¾ç‰‡"
            else:
                doc_dict['image_path'] = ''
            
            # 5. å¯¹äºæ–‡æœ¬å—  æå– title(æ‹¼æ¥æ‰€æœ‰çš„ Header) ä¸å†…å®¹(doc.page_content) å­˜å‚¨åˆ° text å­—æ®µ
            headers = []
            # å‡è®¾ Header çš„é”®å¯èƒ½ä¸º 'Header 1', 'Header 2', 'Header 3' ç­‰ï¼Œæˆ‘ä»¬æŒ‰å±‚çº§é¡ºåºæ‹¼æ¥
            # æˆ‘ä»¬éœ€è¦å…ˆæ”¶é›†æ‰€æœ‰å­˜åœ¨çš„ Header é”®ï¼Œå¹¶æŒ‰å±‚çº§æ’åº
            header_keys = [key for key in metadata.keys() if key.startswith('Header')]              # ['Header 1', 'Header 3']
            # æŒ‰ Header åçš„æ•°å­—æ’åºï¼Œç¡®ä¿å±‚çº§é¡ºåº
            header_keys_sorted = sorted(header_keys, key=lambda x: int(x.split()[1]) if x.split()[1].isdigit() else x)

            for key in header_keys_sorted:
                value = metadata.get(key, '').strip()
                if value:  # åªæ·»åŠ éç©ºçš„ Header å€¼
                    headers.append(value)
            
            # å°†æ‰€æœ‰éç©ºçš„ Header å€¼ç”¨è¿å­—ç¬¦æˆ–ç©ºæ ¼è¿æ¥èµ·æ¥
            doc_dict['title'] = ' --> '.join(headers) if headers else ''  # ä½ ä¹Ÿå¯ä»¥ç”¨å…¶ä»–è¿æ¥ç¬¦ï¼Œå¦‚ç©ºæ ¼
            # å¯¹æ–‡æœ¬å—å¤„ç†ï¼šæ‹¼æ¥æ ‡é¢˜å’Œå†…å®¹
            if metadata.get('embedding_type') == 'text':
                if doc_dict['title']:
                    doc_dict['text'] = doc_dict['title'] + ':' + doc.page_content
                else:
                    doc_dict['text'] = doc.page_content
            
            # 6. å°†doc_dictæ·»åŠ åˆ°result_dictä¸­
            result_dict.append(doc_dict)
            
        return result_dict
    
    def write_to_milvus(self, processed_data: List[Dict]):
        """
        æŠŠæ•°æ®å†™å…¥åˆ°Milvusä¸­
        :param processed_data:
        :return:
        """
        if not processed_data:
            logger.warning("ğŸ¶æ²¡æœ‰éœ€è¦å†™å…¥çš„æ•°æ®")
            return
        
        # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿textå­—æ®µä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        MAX_TEXT_LENGTH = 10000
        for item in processed_data:
            text = item.get('text', '')
            if len(text) > MAX_TEXT_LENGTH:
                logger.warning(f"âš ï¸ æ–‡æœ¬è¶…é•¿({len(text)}å­—ç¬¦)ï¼Œå·²æˆªæ–­è‡³{MAX_TEXT_LENGTH}å­—ç¬¦: {text[:50]}...")
                item['text'] = text[:MAX_TEXT_LENGTH]
        
        try:
            insert_res = self.client.insert(collection_name=COLLECTION_NAME, data=processed_data)
            print(f"[Milvus] æˆåŠŸå†™å…¥ {len(processed_data)} æ¡æ•°æ®.IDs ç¤ºä¾‹: {insert_res['ids'][:5]}")
        except Exception as e:
            logger.error(f"ğŸ¶å†™å…¥Milvuså¤±è´¥: {e}")
            raise e

    @staticmethod
    def generate_image_description(data_list:List[Dict]):
        """
        ä¸ºæ–‡æ¡£ä¸­åŒ…å«å›¾ç‰‡çš„æ¡ç›®(image_path å­—æ®µéç©º)ç”Ÿæˆä¸€æ®µåŸºäºä¸Šä¸‹æ–‡çš„ã€ç®€æ´çš„å¤šæ¨¡æ€æ–‡æœ¬æè¿° ä»¥ä¾¿åç»­å¯ä»¥å°†è¿™æ®µæè¿°ç”¨äºå‘é‡åŒ–(embedding)å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ Milvusã€‚

        å‚æ•°:
            data_list: åŒ…å«å­—å…¸çš„åˆ—è¡¨

        è¿”å›:
            åŒ…å«å®Œæ•´ç»“æœçš„æ–°åˆ—è¡¨
        """
        for index, item in enumerate(data_list):
            if item.get('image_path'):  # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡å­—å…¸
                # è·å–å‰åæ–‡æœ¬å†…å®¹
                prev_text, next_text = get_surrounding_text_content(data_list, index)
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                logger.info(f"\n{'='*50}")
                logger.info(f"æ­£åœ¨å¤„ç†å›¾ç‰‡: {item.get('image_path')}")
                logger.info(f"å‰æ–‡å†…å®¹: {prev_text[:100] if prev_text else 'None'}...")
                logger.info(f"åæ–‡å†…å®¹: {next_text[:100] if next_text else 'None'}...")
                logger.info(f"{'='*50}\n")

                # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64
                base64_img, _  = image_to_base64(item['image_path'])

                # æ„å»ºæç¤ºè¯æ¨¡æ¿
                context_prompt = ""
                if prev_text and next_text:
                    context_prompt = f"""
        ä½ æ˜¯ä¸€ä½ç§‘ç ”è®ºæ–‡å›¾åƒç†è§£ä¸“å®¶ã€‚è¯·åŸºäºè®ºæ–‡ä¸Šä¸‹æ–‡å’Œå›¾ç‰‡å†…å®¹ï¼Œç”Ÿæˆè¯¥å›¾ç‰‡çš„è‹±æ–‡è¯­ä¹‰æè¿°ã€‚

        ã€è®ºæ–‡ä¸Šä¸‹æ–‡ã€‘
        å‰æ–‡ï¼š{prev_text}

        åæ–‡ï¼š{next_text}

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        è¿™æ˜¯ä¸€ç¯‡ç§‘ç ”è®ºæ–‡ä¸­çš„å›¾ç‰‡ï¼Œè¯·ï¼š
        1. **ä¼˜å…ˆå‚è€ƒä¸Šä¸‹æ–‡**ï¼šä»”ç»†é˜…è¯»å‰åæ–‡ï¼Œæå–ä¸å›¾ç‰‡ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å›¾ç‰‡æ ‡é¢˜ã€å›¾æ³¨ã€å®éªŒè¯´æ˜ã€æ•°æ®å«ä¹‰ç­‰ï¼‰
        2. **ç»“åˆå›¾ç‰‡å†…å®¹**ï¼šè§‚å¯Ÿå›¾ç‰‡å®é™…å±•ç¤ºçš„å†…å®¹ï¼ˆå›¾è¡¨ç±»å‹ã€åæ ‡è½´ã€æ•°æ®è¶‹åŠ¿ã€æ¶æ„ç»„æˆç­‰ï¼‰
        3. **ç”Ÿæˆè¯­ä¹‰æè¿°**ï¼šå°†ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å›¾ç‰‡å†…å®¹èåˆï¼Œç”Ÿæˆä¸€æ®µå®Œæ•´ã€å‡†ç¡®çš„æè¿°ï¼Œä½¿è¯»è€…æ— éœ€çœ‹å›¾ä¹Ÿèƒ½ç†è§£å…¶å«ä¹‰
        4. **é‡ç‚¹è¯´æ˜**ï¼š
        - å¦‚æœä¸Šä¸‹æ–‡æåˆ°äº†å›¾å·ã€å›¾é¢˜ï¼Œè¯·åŒ…å«
        - å¦‚æœæ˜¯æ•°æ®å›¾è¡¨ï¼Œè¯´æ˜è¡¨è¾¾çš„æ•°æ®å«ä¹‰å’Œè¶‹åŠ¿
        - å¦‚æœæ˜¯æ¶æ„å›¾/æµç¨‹å›¾ï¼Œè¯´æ˜å…¶å±•ç¤ºçš„ç³»ç»Ÿæˆ–æµç¨‹
        - å¦‚æœæ˜¯å®éªŒåœºæ™¯å›¾ï¼Œè¯´æ˜å®éªŒç¯å¢ƒå’Œå…³é”®è¦ç´ 
        5. æè¿°é•¿åº¦æ§åˆ¶åœ¨200-400å­—

        è¯·ç›´æ¥ç»™å‡ºæè¿°ï¼Œä¸è¦æœ‰"è¿™å¼ å›¾ç‰‡..."ç­‰å‰ç¼€ã€‚
                            """
                elif prev_text:
                    context_prompt = f"""
        ä½ æ˜¯ä¸€ä½ç§‘ç ”è®ºæ–‡å›¾åƒç†è§£ä¸“å®¶ã€‚è¯·åŸºäºè®ºæ–‡ä¸Šä¸‹æ–‡å’Œå›¾ç‰‡å†…å®¹ï¼Œç”Ÿæˆè¯¥å›¾ç‰‡çš„è‹±æ–‡è¯­ä¹‰æè¿°ã€‚

        ã€è®ºæ–‡ä¸Šä¸‹æ–‡ï¼ˆå‰æ–‡ï¼‰ã€‘
        {prev_text}

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        è¿™æ˜¯ä¸€ç¯‡ç§‘ç ”è®ºæ–‡ä¸­çš„å›¾ç‰‡ï¼Œè¯·ï¼š
        1. **ä¼˜å…ˆå‚è€ƒå‰æ–‡**ï¼šä»”ç»†é˜…è¯»å‰æ–‡ï¼Œæå–ä¸å›¾ç‰‡ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å›¾ç‰‡æ ‡é¢˜ã€å›¾æ³¨ã€å®éªŒè¯´æ˜ç­‰ï¼‰
        2. **ç»“åˆå›¾ç‰‡å†…å®¹**ï¼šè§‚å¯Ÿå›¾ç‰‡å®é™…å±•ç¤ºçš„å†…å®¹
        3. **ç”Ÿæˆè¯­ä¹‰æè¿°**ï¼šå°†ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å›¾ç‰‡å†…å®¹èåˆï¼Œç”Ÿæˆä¸€æ®µå®Œæ•´ã€å‡†ç¡®çš„æè¿°
        4. **é‡ç‚¹è¯´æ˜**ï¼šå›¾å·ã€å›¾é¢˜ã€æ•°æ®å«ä¹‰ã€æ¶æ„ç»„æˆæˆ–å®éªŒè¦ç´ 
        5. æè¿°é•¿åº¦æ§åˆ¶åœ¨200-400å­—

        è¯·ç›´æ¥ç»™å‡ºæè¿°ï¼Œä¸è¦æœ‰"è¿™å¼ å›¾ç‰‡..."ç­‰å‰ç¼€ã€‚
                            """
                elif next_text:
                    context_prompt = f"""
        ä½ æ˜¯ä¸€ä½ç§‘ç ”è®ºæ–‡å›¾åƒç†è§£ä¸“å®¶ã€‚è¯·åŸºäºè®ºæ–‡ä¸Šä¸‹æ–‡å’Œå›¾ç‰‡å†…å®¹ï¼Œç”Ÿæˆè¯¥å›¾ç‰‡çš„è‹±æ–‡è¯­ä¹‰æè¿°ã€‚

        ã€è®ºæ–‡ä¸Šä¸‹æ–‡ï¼ˆåæ–‡ï¼‰ã€‘
        {next_text}

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        è¿™æ˜¯ä¸€ç¯‡ç§‘ç ”è®ºæ–‡ä¸­çš„å›¾ç‰‡ï¼Œè¯·ï¼š
        1. **ä¼˜å…ˆå‚è€ƒåæ–‡**ï¼šä»”ç»†é˜…è¯»åæ–‡ï¼Œæå–ä¸å›¾ç‰‡ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å›¾ç‰‡è¯´æ˜ã€ç»“æœåˆ†æç­‰ï¼‰
        2. **ç»“åˆå›¾ç‰‡å†…å®¹**ï¼šè§‚å¯Ÿå›¾ç‰‡å®é™…å±•ç¤ºçš„å†…å®¹
        3. **ç”Ÿæˆè¯­ä¹‰æè¿°**ï¼šå°†ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å›¾ç‰‡å†…å®¹èåˆï¼Œç”Ÿæˆä¸€æ®µå®Œæ•´ã€å‡†ç¡®çš„æè¿°
        4. **é‡ç‚¹è¯´æ˜**ï¼šå›¾å·ã€å›¾é¢˜ã€æ•°æ®å«ä¹‰ã€æ¶æ„ç»„æˆæˆ–å®éªŒè¦ç´ 
        5. æè¿°é•¿åº¦æ§åˆ¶åœ¨200-400å­—

        è¯·ç›´æ¥ç»™å‡ºæè¿°ï¼Œä¸è¦æœ‰"è¿™å¼ å›¾ç‰‡..."ç­‰å‰ç¼€ã€‚
                            """
                else:
                    context_prompt = """
        ä½ æ˜¯ä¸€ä½ç§‘ç ”è®ºæ–‡å›¾åƒç†è§£ä¸“å®¶ã€‚è¯·è§‚å¯Ÿè¿™å¼ å›¾ç‰‡å¹¶ç”Ÿæˆè‹±æ–‡æè¿°ã€‚

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        è¿™æ˜¯ä¸€ç¯‡ç§‘ç ”è®ºæ–‡ä¸­çš„å›¾ç‰‡ï¼Œè¯·ï¼š
        1. è¯†åˆ«å›¾ç‰‡ç±»å‹ï¼ˆæ•°æ®å›¾è¡¨ã€æ¶æ„å›¾ã€æµç¨‹å›¾ã€å®éªŒåœºæ™¯å›¾ç­‰ï¼‰
        2. æè¿°å›¾ç‰‡å±•ç¤ºçš„æ ¸å¿ƒå†…å®¹å’Œå…³é”®ä¿¡æ¯
        3. å¦‚æœæ˜¯å›¾è¡¨ï¼Œè¯´æ˜åæ ‡è½´ã€æ•°æ®è¶‹åŠ¿ç­‰
        4. å¦‚æœæ˜¯æ¶æ„/æµç¨‹å›¾ï¼Œè¯´æ˜ä¸»è¦ç»„æˆéƒ¨åˆ†
        5. æè¿°é•¿åº¦æ§åˆ¶åœ¨200-400å­—

        è¯·ç›´æ¥ç»™å‡ºæè¿°ï¼Œä¸è¦æœ‰"è¿™å¼ å›¾ç‰‡..."ç­‰å‰ç¼€ã€‚
                            """

                # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                message = HumanMessage(
                    content=[
                        {"type": "text", "text": context_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"{base64_img}"
                            }
                        }
                    ]
                )

                # è°ƒç”¨æ¨¡å‹ç”Ÿæˆæè¿°  ä¿®æ”¹åŸå§‹ç±»å‹ä¸ºå›¾ç‰‡çš„textå­—æ®µ
                response = qwen3_max.invoke([message])
                item['text'] = response.content
        
        return data_list

    def do_save_to_milvus(self, processed_data: List[Document]):
        """
        ç¬¬ä¸€æ­¥ï¼š
        æŠŠSplitterä¹‹åçš„çš„æ•°æ®ï¼ˆdocumentå¯¹è±¡åˆ—è¡¨ï¼‰ï¼Œå…ˆè½¬æ¢ä¸ºå­—å…¸ï¼›
        ç¬¬äºŒæ­¥ï¼š
        æŠŠå­—å…¸ä¸­çš„æ–‡æœ¬ å’Œå›¾ç‰‡ ï¼Œè¿›è¡Œå‘é‡åŒ–ï¼Œç„¶åå†å­˜å…¥å­—å…¸ã€‚
        ç¬¬ä¸‰æ­¥ï¼š
        æœ€åå†™å…¥å‘é‡æ•°æ®åº“
        :param processed_data:
        :return:
        """
        # ç¬¬ä¸€æ­¥
        expanded_data = MilvusVectorSave.generate_image_description(MilvusVectorSave.doc_to_dict(processed_data))
        processed_data: List[Dict] = []
        # å¤„ç†æ¯ä¸ª item
        for idx, item in enumerate(expanded_data, 1):
            # é™é€Ÿæ§åˆ¶
            limiter.acquire()

            # å¤„ç† + å¯é€‰ 429 é‡è¯•
            if RETRY_ON_429:
                attempts = 0
                while True:
                    # åŒ…è£…ç‰ˆå¤„ç†
                    result = process_item_with_guard(item.copy())
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                    if result.get("text_content_dense"):
                        processed_data.append(result)
                        break
                    attempts += 1
                    if attempts > MAX_429_RETRIES:
                        print(f"[429é‡è¯•] è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡ idx={idx}")
                        processed_data.append(result)
                        break
                    backoff = BASE_BACKOFF * (2 ** (attempts - 1)) * (0.8 + random.random() * 0.4)
                    print(f"[429é‡è¯•] ç¬¬{attempts}æ¬¡ï¼Œsleep {backoff:.2f}s â€¦")
                    time.sleep(backoff)
            else:
                # è‹¥å…³é—­ 429 é‡è¯•ï¼Œè¿™é‡ŒåŒæ ·ä½¿ç”¨åŒ…è£…ç‰ˆ
                processed_data.append(process_item_with_guard(item.copy()))

            # è¿›åº¦æ‰“å°
            if idx % 20 == 0:
                print(f"[è¿›åº¦] å·²å¤„ç† {idx}/{len(expanded_data)}")

        # æ‰“å°å¤„ç†åçš„ item å†…å®¹
        # for item in processed_data:
        #     print(json.dumps(item, ensure_ascii=False, indent=4))
        
        # ç¬¬ä¸‰æ­¥ï¼šå†™å…¥å‘é‡æ•°æ®åº“
        self.write_to_milvus(processed_data)
        
        # è¿”å›å¤„ç†åçš„æ•°æ®
        return processed_data

if __name__ == "__main__":

    # åˆ›å»ºè¡¨ç»“æ„
    milvus_vector_save = MilvusVectorSave()
    # milvus_vector_save.create_collection(is_first=True)
    
    milvus_vector_save.create_context_collection(is_first=True)
    # æŸ¥çœ‹é›†åˆä¿¡æ¯
    # client = MilvusClient(uri=MILVUS_URI, user='root', password='Milvus')
    # res = client.describe_collection(collection_name=COLLECTION_NAME)
    # print("é›†åˆä¿¡æ¯:")
    # print(res)
    # md_dir = r"F:\workspace\langgraph_project\Multimodal_RAG\output\GPT4æŠ€æœ¯æŠ¥å‘Š"
    # splitter = MarkdownDirSplitter(images_output_dir=r"F:\workspace\langgraph_project\Multimodal_RAG\output\images")
    # docs = splitter.process_md_dir(md_dir, source_filename="GPT4æŠ€æœ¯æŠ¥å‘Š.pdf")

    # res: List[Dict] = milvus_vector_save.do_save_to_milvus(docs)

    # # æ‰“å°
    # # æ‰“å°å…³é”®æ•°æ®
    # for i, item in enumerate(res):
    #     print(f"\n==== ç¬¬{i+1}æ¡æ•°æ® ====")
    #     # æ‰“å°æ–‡æœ¬å†…å®¹å‰30å­—
    #     text = item.get('text', '')
    #     print(f"å†…å®¹: {text[:30]}{'...' if len(text) > 30 else ''}")
    #     # æ‰“å°æ ‡é¢˜
    #     print(f"æ ‡é¢˜: {item.get('title', '')}")
    #     # æ‰“å°æ–‡ä»¶åã€æ–‡ä»¶ç±»å‹
    #     print(f"æ–‡ä»¶å: {item.get('filename', '')}")
    #     print(f"æ–‡ä»¶ç±»å‹: {item.get('filetype', '')}")

